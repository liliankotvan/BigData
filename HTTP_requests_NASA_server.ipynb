{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# HTTP requests to the NASA Kennedy Space Center WWW server\n",
    "Fonte oficial do dateset : http://ita.ee.lbl.gov/html/contrib/NASA-HTTP.html\n",
    "\n",
    "Dados :\n",
    "\n",
    "● Jul 01 to Jul 31, ASCII format, 20.7 MB gzip compressed , 205.2 MB.\n",
    "\n",
    "● Aug 04 to Aug 31, ASCII format, 21.8 MB gzip compressed , 167.8 MB.\n",
    "\n",
    "Sobre o dataset : Esses dois conjuntos de dados possuem todas as requisições HTTP para o servidor da NASA Kennedy\n",
    "Space Center WWW na Flórida para um período específico.\n",
    "Os logs estão em arquivos ASCII com uma linha por requisição com as seguintes colunas:\n",
    "\n",
    "● Host fazendo a requisição . Um hostname quando possível, caso contrário o endereço de internet se o nome\n",
    "não puder ser identificado.\n",
    "\n",
    "● Timestamp no formato \"DIA/MÊS/ANO:HH:MM:SS TIMEZONE\"\n",
    "\n",
    "● Requisição (entre aspas)\n",
    "\n",
    "● Código do retorno HTTP\n",
    "\n",
    "● Total de bytes retornados\n",
    "\n",
    "Questões:\n",
    "Responda as seguintes questões devem ser desenvolvidas em Spark utilizando a sua linguagem de preferência.\n",
    "\n",
    "1. Número de hosts únicos.\n",
    "2. O total de erros 404.\n",
    "3. Os 5 URLs que mais causaram erro 404.\n",
    "4. Quantidade de erros 404 por dia.\n",
    "5. O total de bytes retornados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://CPX-584B39T7CCV.dir.svc.accenture.com:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.5</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificando se o SparkContext foi iniciado - inicia automaticamente quando o \n",
    "#jupyter notebook para pyspark é aberto pela linha de comando na pasta do projeto.\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<SparkContext master=local[*] appName=PySparkShell>\n",
      "2.4.5\n"
     ]
    }
   ],
   "source": [
    "#Verificando os dados do SparkContext (master e appName)\n",
    "print(sc)\n",
    "\n",
    "#Verificando a versão utilizada \n",
    "print(sc.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x0000023481F729E8>\n"
     ]
    }
   ],
   "source": [
    "#Importando sql\n",
    "from pyspark import sql \n",
    "from pyspark.sql import *\n",
    "\n",
    "#Iniciando a sessão\n",
    "spark = SparkSession.builder.master(\"local\").appName(\"RespostaLilian\").getOrCreate()\n",
    "sc = SparkContext.getOrCreate()\n",
    "\n",
    "#Verificando a sessão do SparkSession\n",
    "print(spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "linhas_arquivo_julho =  1891715\n",
      "linhas_arquivo_agosto =  1569898\n",
      "linhas_arquivo_total =  3461613\n"
     ]
    }
   ],
   "source": [
    "#Carregando os arquivos e unindo ambos em um único Dataframe com o nome de 'df'\n",
    "data_jul = spark.read.load(\"C:\\\\Spark\\\\projetos\\\\dataset\\\\NASA_access_log_Jul95.gz\",\n",
    "                     format=\"text\", sep=\",\", inferSchema=\"true\", header=\"false\") \n",
    "\n",
    "data_aug = spark.read.load(\"C:\\\\Spark\\\\projetos\\\\dataset\\\\NASA_access_log_Aug95.gz\",\n",
    "                     format=\"text\", sep=\",\", inferSchema=\"true\", header=\"false\") \n",
    "\n",
    "df = data_jul.union(data_aug)\n",
    "\n",
    "#Conferindo o número de linhas dos arquivos separados e após a união\n",
    "print('linhas_arquivo_julho = ', data_jul.count())\n",
    "\n",
    "print('linhas_arquivo_agosto = ', data_aug.count())\n",
    "\n",
    "print('linhas_arquivo_total = ', df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- value: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Conferindo o tipo do df\n",
    "type(df)\n",
    "\n",
    "#Conferindo o schema do df\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='199.72.81.55 - - [01/Jul/1995:00:00:01 -0400] \"GET /history/apollo/ HTTP/1.0\" 200 6245'),\n",
       " Row(value='unicomp6.unicomp.net - - [01/Jul/1995:00:00:06 -0400] \"GET /shuttle/countdown/ HTTP/1.0\" 200 3985'),\n",
       " Row(value='199.120.110.21 - - [01/Jul/1995:00:00:09 -0400] \"GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0\" 200 4085'),\n",
       " Row(value='burger.letters.com - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/countdown/liftoff.html HTTP/1.0\" 304 0'),\n",
       " Row(value='199.120.110.21 - - [01/Jul/1995:00:00:11 -0400] \"GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0\" 200 4179')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Verificando as 5 primeiras linhas do df\n",
    "df.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(value='199.72.81.55,01/Jul/1995:00:00:01 -0400,GET /history/apollo/ HTTP/1.0,200,6245'),\n",
       " Row(value='unicomp6.unicomp.net,01/Jul/1995:00:00:06 -0400,GET /shuttle/countdown/ HTTP/1.0,200,3985'),\n",
       " Row(value='199.120.110.21,01/Jul/1995:00:00:09 -0400,GET /shuttle/missions/sts-73/mission-sts-73.html HTTP/1.0,200,4085'),\n",
       " Row(value='burger.letters.com,01/Jul/1995:00:00:11 -0400,GET /shuttle/countdown/liftoff.html HTTP/1.0,304,0'),\n",
       " Row(value='199.120.110.21,01/Jul/1995:00:00:11 -0400,GET /shuttle/missions/sts-73/sts-73-patch-small.gif HTTP/1.0,200,4179')]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Importando as funções do sql para utilizar regex\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "#Separando por vírgula as colunas desejadas do df - utilizando regex\n",
    "df_sep = df.withColumn('value', regexp_replace('value', '(.*) - - \\[(.*)\\] \"(.*)\" (\\d*) (.*)', '$1,$2,$3,$4,$5'))\n",
    "\n",
    "df_sep.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+----+-----+\n",
      "|                Host|                Time|                 Req|Code|Total|\n",
      "+--------------------+--------------------+--------------------+----+-----+\n",
      "|        199.72.81.55|01/Jul/1995:00:00...|GET /history/apol...| 200| 6245|\n",
      "|unicomp6.unicomp.net|01/Jul/1995:00:00...|GET /shuttle/coun...| 200| 3985|\n",
      "|      199.120.110.21|01/Jul/1995:00:00...|GET /shuttle/miss...| 200| 4085|\n",
      "|  burger.letters.com|01/Jul/1995:00:00...|GET /shuttle/coun...| 304|    0|\n",
      "|      199.120.110.21|01/Jul/1995:00:00...|GET /shuttle/miss...| 200| 4179|\n",
      "|  burger.letters.com|01/Jul/1995:00:00...|GET /images/NASA-...| 304|    0|\n",
      "|  burger.letters.com|01/Jul/1995:00:00...|GET /shuttle/coun...| 200|    0|\n",
      "|     205.212.115.106|01/Jul/1995:00:00...|GET /shuttle/coun...| 200| 3985|\n",
      "|         d104.aa.net|01/Jul/1995:00:00...|GET /shuttle/coun...| 200| 3985|\n",
      "|      129.94.144.152|01/Jul/1995:00:00...|      GET / HTTP/1.0| 200| 7074|\n",
      "+--------------------+--------------------+--------------------+----+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Separando as colunas e nomeando-as\n",
    "col_names = (\"Host\",\"Time\",\"Req\",\"Code\",\"Total\")\n",
    "\n",
    "df_col = df_sep.withColumn(\"value\", split(\"value\",\",\"))\n",
    "\n",
    "df_name = df_col.select(*(col('value').getItem(i).alias(col_names[i]) for i in range(5)))\n",
    "\n",
    "df_name.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Host: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Req: string (nullable = true)\n",
      " |-- Code: string (nullable = true)\n",
      " |-- Total: string (nullable = true)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3461613"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Conferindo o schema e a contagem\n",
    "df_name.printSchema()\n",
    "\n",
    "df_name.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Host: string (nullable = true)\n",
      " |-- Time: string (nullable = true)\n",
      " |-- Req: string (nullable = true)\n",
      " |-- Code: string (nullable = true)\n",
      " |-- Total: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Fazendo a alteração do schema\n",
    "df_name = df_name.select(df_name.Host, \n",
    "                         df_name.Time,\n",
    "                         df_name.Req,\n",
    "                         df_name.Code,\n",
    "                         df_name.Total.cast(\"integer\")\n",
    "                        )\n",
    "\n",
    "df_name.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Criando uma tabela temporária para poder trabalhar com spark sql\n",
    "df_name.createOrReplaceTempView('tabela')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|No_hosts_unicos|\n",
      "+---------------+\n",
      "|         137979|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Respondendo questão 1\n",
    "spark.sql(\"SELECT COUNT(DISTINCT Host) AS No_hosts_unicos FROM tabela\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|Total_erros_404|\n",
      "+---------------+\n",
      "|          20848|\n",
      "+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Respondendo a questão 2\n",
    "spark.sql(\"SELECT COUNT(Code) AS Total_erros_404 FROM tabela WHERE Code==404\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-----------+\n",
      "|                Host|count(Host)|\n",
      "+--------------------+-----------+\n",
      "|hoohoo.ncsa.uiuc.edu|        251|\n",
      "|piweba3y.prodigy.com|        157|\n",
      "|jbiagioni.npt.nuw...|        132|\n",
      "|piweba1y.prodigy.com|        114|\n",
      "|www-d4.proxy.aol.com|         91|\n",
      "+--------------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Respondendo a questão 3\n",
    "spark.sql(\"SELECT Host,COUNT(Host) FROM tabela WHERE Code==404 GROUP BY Host ORDER BY COUNT(Host) DESC \").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----+\n",
      "|       Date|count|\n",
      "+-----------+-----+\n",
      "|02/Jul/1995|  291|\n",
      "|21/Aug/1995|  305|\n",
      "|06/Aug/1995|  371|\n",
      "|16/Jul/1995|  257|\n",
      "|07/Aug/1995|  537|\n",
      "|11/Aug/1995|  263|\n",
      "|27/Jul/1995|  336|\n",
      "|07/Jul/1995|  566|\n",
      "|17/Jul/1995|  403|\n",
      "|15/Jul/1995|  254|\n",
      "|18/Jul/1995|  465|\n",
      "|26/Jul/1995|  336|\n",
      "|03/Aug/1995|  304|\n",
      "|18/Aug/1995|  256|\n",
      "|17/Aug/1995|  270|\n",
      "|14/Aug/1995|  287|\n",
      "|10/Jul/1995|  398|\n",
      "|04/Jul/1995|  358|\n",
      "|20/Aug/1995|  311|\n",
      "|20/Jul/1995|  419|\n",
      "+-----------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Respondendo a questão 4 (mostrando as primeiras 20 linhas)\n",
    "numero4 = df_name.select(df_name.Time.substr(0,11).alias(\"Date\"),\"Code\")\n",
    "\n",
    "numero4 = numero4.filter(numero4.Code.like('404'))\n",
    "\n",
    "numero4 = numero4.groupBy(\"Date\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n",
      "|total_bytes_retornados|\n",
      "+----------------------+\n",
      "|           65535001345|\n",
      "+----------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Respondendo a questão 5\n",
    "spark.sql(\"SELECT SUM(Total) AS total_bytes_retornados FROM tabela\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
