{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perguntas e Respostas : Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1- Qual o objetivo do comando cache em Spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R1: \n",
    "O objetivo é acelerar os trabalhos com o armazenamento de resultados intermediários em cache.\n",
    "Isto é muito útil quando os dados são acessados repetidamente. O Spark possui os seus próprios métodos cache\n",
    "nativos como .persist(), .cache() e CACHE TABLE, normalmente utilizado em pequenos conjuntos de dados e que \n",
    "não funcionam com particionamento."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2- O mesmo código implementado em Spark é normalmente mais rápido que a implementação equivalente em MapReduce. Por quê?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R2: \n",
    "O MapReduce é implementado diretamente pelo Hadoop, ou seja, faz o processamento (leitura e gravação)em disco.\n",
    "Já o Spark faz o processamento em memória, aumentando muito a velocidade de processamento, podendo ser até 100x\n",
    "mais rápido que o MapReduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3- Qual é a função do SparkContext ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R3: \n",
    "O SparkContext é o ponto de entrada do projeto para as funcionalidades do Spark. Toda aplicação deve conter esse\n",
    "objeto para interagir com o cluster do Spark ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4- Explique com suas palavras o que é Resilient Distributed Datasets (RDD)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R4: \n",
    "RDD é uma coleção de elementos que são operados no cluster em paralelo e também são tolerantes a falhas. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5- GroupByKey é menos eficiente que reduceByKey em grandes dataset. Por quê?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R5:\n",
    "ReduceByKey - é uma operação de transformação no Spark. Antes de enviar os dados pelas partições, realiza o merge localmente e então envia os elementos resultantes como em GroupByKey. ReduceByKey trabalha mergeando os valores de cada chave usando uma função redutiva associativa e comutativa, similarmente ao que ocorre na programação MapReduce. \n",
    "\n",
    "GroupByKey - é uma operação de transformação que promove o embaralhamento dos dados. Recebe os pares chave-valor de entrada (k,v), agrupa os valores baseado na chave e gera um dataset de (k, iterável) par de saída. É uma operação cara por todo embaralhamento necessário. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6- Explique o que o código Scala abaixo faz.\n",
    "1.        val textFile = sc.textFile( \"hdfs://...\" )\n",
    "2.        val counts = textFile.flatMap(line => line.split(\" \"))\n",
    "3.                    .map(word => (word,1))\n",
    "4.                    .reduceByKey(_+_)\n",
    "5.        counts.saveAsTextFile(\"hdfs://...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# R6:\n",
    "linha 1 - realizando a leitura do arquivo de texto a partir de um caminho do hdfs\n",
    "linha 2 - separando as palavras por \" \"\n",
    "linha 3 - dando o valor de 1 para cada palavra\n",
    "linha 4 - somando a quantidade de palavras\n",
    "linha 5 - salvando o resultado (quantidade de palavras) em um arquivo de texto no hdfs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
